import java.io.BufferedReader;
import java.io.File;
import java.io.FileReader;
import java.io.IOException;
import java.net.URI;
import java.util.HashMap;

import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Reducer;

public class Q2Reducer extends Reducer<IntWritable, Text, Text	, IntWritable>{
	int i=0;
	
	HashMap<String, String> moviesMap=new HashMap<String,String>();

	
	@Override
	protected void setup(Reducer<IntWritable, Text, Text, IntWritable>.Context context)
			throws IOException, InterruptedException {

		URI[] files=context.getCacheFiles();
		for (int i = 0; i < files.length; i++) {
			System.out.println("++++++++++++++++++++"+files[i].toString());
		}
		
		File f=new File("movies.dat");
		BufferedReader reader=new BufferedReader(new FileReader(f));
		String line="";
		try{
			while((line=reader.readLine())!=null){
				String mvid=line.split("::")[0];
				String mvname=line.split("::")[1];
				moviesMap.put(mvid, mvname);
			}		
		}finally{
			if(reader!=null){
				reader.close();
			}
		}
		
	}
	
	@Override
	protected void reduce(IntWritable key, Iterable<Text> values,
			Reducer<IntWritable, Text, Text, IntWritable>.Context context) throws IOException, InterruptedException {
		
			for (Text val : values) {
				if(i<10){
				context.write(val,new IntWritable(key.get()*-1));
				i=i+1;
			}			
		}
			
	
	}

}
